### Visual-Monocular-SLAM-Project
## Document Overview
The following GitHub repository is a collection of details about visual monocular simultaneous localization and mapping. The original code is completed and installed as an example by MATLAB themselves, which is referenced [here](https://github.com/frankzachma/Visual-Mono-SLAM/blob/main/SLAM/Step_2/Deliverables.md). In order for the code to work for a custom dataset, there were edits made to the code, which is discused in detail [here](https://github.com/frankzachma/Visual-Mono-SLAM/blob/main/SLAM/Step_2/Deliverables.md). The custom [dataset](https://github.com/frankzachma/Visual-Mono-SLAM/tree/main/SLAM/Step_2/Pictures) in questions contains pictures taken of a classroom lectern using a laptop camera. 

## Summary of MATLAB's Visual Monocular SLAM using ORB-SLAM
The monocular visual simultaneous localization and mapping example from MATLAB shows a visual SLAM system using monocular camera images. This example combines feature detection, feature matching, pose estimation, and map generation to build a point cloud map of the environment.

The camera parameters, specifically the intrinsic camera parameters, are used to estimate the relative rotation and translation between the first two images of the dataset to establish a baseline. Along with these parameters, the map initialization block of code generates features for both pictures and then matches the features that are the same in both pictures. Oriented FAST and rotated BRIEF (ORB) is the feature detection algorthim that is used to pick out various feature points. If the scene is planar, the homography projective transformation is used to describe the motion between frames using features that match. If the scene is not planar, the fundamental matrix is used instead. 

Triangulation: Once the camera motion is estimated, 3D points corresponding to the matched features are triangulated. This process involves estimating the depth of each feature point using the known camera pose and feature correspondences.
Bundle Adjustment: To improve the accuracy of the estimated camera poses and 3D points, bundle adjustment is performed. Bundle adjustment optimizes the camera poses and 3D points to minimize reprojection errors across all frames.
Loop Closure: In larger environments, loop closure detection may be necessary to detect when the camera revisits a previously visited location. Loop closure detection helps in correcting accumulated drift and refining the map.
Map Visualization: Finally, the map and camera trajectory are visualized to provide insights into the SLAM system's performance. This visualization can include the reconstructed 3D map, camera poses, and feature tracks.
Overall, the Monocular Visual SLAM example from MATLAB demonstrates a basic implementation of a visual SLAM system using monocular camera images. It's a simplified version of more complex SLAM algorithms used in practical applications, but it serves as a good starting point for understanding the fundamental concepts of visual SLAM.
